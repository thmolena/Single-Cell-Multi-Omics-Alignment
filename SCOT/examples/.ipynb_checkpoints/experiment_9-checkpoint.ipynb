{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fc1cb-1205-4dfb-b670-73625b38bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import metric_learn\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# For optimal transport operations:\n",
    "import ot\n",
    "# For computing graph distances:\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# For pre-processing, normalization\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0b841-d0fe-4c1a-b097-b24c626f6f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d28a8-60c2-464e-9ef9-0b08a7e8bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrna=np.load(\"../data/scrna_feat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da3367-f12d-4ccf-9a44-90b05377eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrna.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88433a12-6ecf-404d-8635-eecd0d71b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logarithmic transformation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create a new matrix to store the transformed values\n",
    "scatac = np.zeros((1047, 10))\n",
    "\n",
    "# Apply the sigmoid function to each element of the original matrix\n",
    "for i in range(100):\n",
    "    for j in range(10):\n",
    "        scatac[i, j] = sigmoid(scrna[i, j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae188f-0813-4ae9-8667-10cd6ffdc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8056d34-0e29-4c99-b0fe-186447951fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2  = normalize(scatac, norm=\"l2\"), normalize(scrna, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951c2a6-67f4-40bd-b469-b8fde461ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.zeros((X1.shape[0],X1.shape[0]))\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X1.shape[0]):\n",
    "        C1[i][j] = np.linalg.norm(X1[i][:] - X1[j][:],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c420b3-d7ce-46ba-bb50-2bc2629072ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "C2 = np.zeros((X2.shape[0],X2.shape[0]))\n",
    "for i in range(X2.shape[0]):\n",
    "    for j in range(X2.shape[0]):\n",
    "        C2[i][j] = np.linalg.norm(X2[i][:] - X2[j][:],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8e151-3917-4dee-9689-fd08b146beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = C1/C1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea1639-12ac-4623-afb1-1b8f258fbec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C2 = C2/C2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28237743-ea1d-4463-ae1d-2feb23af7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ot\n",
    "import torch.nn.functional as F\n",
    "from geomloss import SamplesLoss\n",
    "from torch import nn, optim\n",
    "import pytorch_metric_learning.losses as loss\n",
    "import pytorch_metric_learning.miners as miners\n",
    "import matplotlib.pylab as pl\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import procrustes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def min_gromov_wasserstein_distance(C1_fixed, C2_fixed, C1, C2, p, q, nb_iter_max=10, lr=1e-2):\n",
    "    \n",
    "    loss_iter = []\n",
    "    loss_iter1 = []\n",
    "    optimizer = torch.optim.SGD([C1,C2], lr=0.02)\n",
    "\n",
    "    for i in range(nb_iter_max):\n",
    "        optimizer.zero_grad()\n",
    "        loss1 = ot.gromov.entropic_gromov_wasserstein2(C1, C2, p, q, loss_fun=\"square_loss\", epsilon=0.1, max_iter=1000, tol=1e-09, verbose=False, log=False)\n",
    "        \n",
    "        \n",
    "        loss4 = ot.gromov.entropic_gromov_wasserstein2(C1_fixed, C1, p, q, loss_fun=\"square_loss\", epsilon=1e-6, max_iter=1000, tol=1e-09, verbose=False, log=False)\n",
    "\n",
    "        loss5 = ot.gromov.entropic_gromov_wasserstein2(C2_fixed, C2, p, q, loss_fun=\"square_loss\", epsilon=1e-8, max_iter=1000, tol=1e-09, verbose=False, log=False)\n",
    "        \n",
    "        \n",
    "    \n",
    "        loss = 0*loss1+loss4+loss5\n",
    "        \n",
    "        loss_iter1.append(loss4.clone().detach().cpu().numpy())\n",
    "        loss_iter.append(loss.clone().detach().cpu().numpy())\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "  \n",
    "        # Project the updated arrays onto the probability simplex\n",
    "        C1 = ot.utils.proj_simplex(C1)\n",
    "        #print(C1)\n",
    "        C2 = ot.utils.proj_simplex(C2)\n",
    "        #print(C2)\n",
    "\n",
    "        # Convert the projected arrays back to PyTorch tensors\n",
    "        C1 = torch.tensor(C1, requires_grad=True)\n",
    "        C2 = torch.tensor(C2, requires_grad=True)\n",
    "        \n",
    "        optimizer.step()\n",
    "  \n",
    "\n",
    "    # Convert the final tensors back to numpy arrays\n",
    "    C1 = C1.detach().cpu().numpy()\n",
    "    C2 = C2.detach().cpu().numpy()\n",
    "    \n",
    "    return C1, C2, loss_iter, loss_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83095035-5fe9-4d0a-800e-95fd0799a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import ot\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "C1 = torch.tensor(C1, dtype=torch.float64, requires_grad=True)\n",
    "C2 = torch.tensor(C2, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p = ot.unif(C1.shape[0])\n",
    "p = torch.tensor(p)\n",
    "q = ot.unif(C2.shape[0])\n",
    "q = torch.tensor(q)\n",
    "C1_opt, C2_opt, loss_iter, loss_iter1 = min_gromov_wasserstein_distance(C1, C2, C1, C2, p, q, nb_iter_max=5, lr=1e-2)\n",
    "    \n",
    "\n",
    "pl.figure(2)\n",
    "pl.plot(loss_iter)\n",
    "pl.title(\"Loss along iterations\")\n",
    "\n",
    "pl.plot(loss_iter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c929b-8e3e-44ad-9fea-da112b9a2182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868867bb-2eb3-4033-b004-771616d2b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1 = C1.detach().cpu().numpy()\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e1b65-1a9c-40ef-b1f0-bb600ec2f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2 = C2.detach().cpu().numpy()\n",
    "print(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc03299-fdab-4f39-a4b2-7ced8b5c4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7549ba-cf8c-4f23-84a0-c3333bfd8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ot.unif(C1.shape[0])\n",
    "q = ot.unif(C2.shape[0])\n",
    "distance_before = ot.gromov.entropic_gromov_wasserstein2(C1, C2, p, q, loss_fun = 'square_loss', epsilon=0.1)\n",
    "print(distance_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5af5e-1d47-4dd0-9b52-2e0b8efae5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad2fdc-5e0c-4d6f-9c4a-41f4801be646",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ot.unif(C1_opt.shape[0])\n",
    "q = ot.unif(C1_opt.shape[0])\n",
    "distance_after = ot.gromov.entropic_gromov_wasserstein2(C1_opt, C2_opt, p, q, loss_fun = 'square_loss', epsilon=0.1)\n",
    "print(distance_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982f875-dc5c-427a-825d-1cdb5c7d1ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747dcb5-4332-4a42-b54f-47546464ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ot.unif(C1_opt.shape[0])\n",
    "q = ot.unif(C2_opt.shape[0])\n",
    "P, log= ot.gromov.entropic_gromov_wasserstein(C1_opt, C2_opt, p, q, loss_fun='square_loss', epsilon=0.1, log=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab5154-9a46-4b6a-8de3-7160cbc8b646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47cdd9-a2ec-4e86-b5eb-a764d672b467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f896656-14b0-4929-bb7a-81e0d4c3b6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499620ad-3c43-4808-b784-7feede6b468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval function\n",
    "\n",
    "import numpy as np\n",
    "import random, math, os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import roc_auc_score, silhouette_samples\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def calc_frac_idx(x1_mat,x2_mat):\n",
    "    \"\"\"\n",
    "    Returns fraction closer than true match for each sample (as an array)\n",
    "    \"\"\"\n",
    "    fracs = []\n",
    "    x = []\n",
    "    nsamp = x1_mat.shape[0]\n",
    "    rank=0\n",
    "    for row_idx in range(nsamp):\n",
    "        euc_dist = np.sqrt(np.sum(np.square(np.subtract(x1_mat[row_idx,:], x2_mat)), axis=1))\n",
    "        true_nbr = euc_dist[row_idx]\n",
    "        sort_euc_dist = sorted(euc_dist)\n",
    "        rank =sort_euc_dist.index(true_nbr)\n",
    "        frac = float(rank)/(nsamp -1)\n",
    "\n",
    "        fracs.append(frac)\n",
    "        x.append(row_idx+1)\n",
    "\n",
    "    return fracs,x\n",
    "\n",
    "def calc_domainAveraged_FOSCTTM(x1_mat, x2_mat):\n",
    "    \"\"\"\n",
    "    Outputs average FOSCTTM measure (averaged over both domains)\n",
    "    Get the fraction matched for all data points in both directions\n",
    "    Averages the fractions in both directions for each data point\n",
    "    \"\"\"\n",
    "    \n",
    "    fracs1,xs = calc_frac_idx(x1_mat, x2_mat)\n",
    "    fracs2,xs = calc_frac_idx(x2_mat, x1_mat)\n",
    "    fracs = []\n",
    "    for i in range(len(fracs1)):\n",
    "        fracs.append((fracs1[i]+fracs2[i])/2)  \n",
    "    return fracs\n",
    "\n",
    "def calc_sil(x1_mat,x2_mat,x1_lab,x2_lab):\n",
    "    \"\"\"\n",
    "    Returns silhouette score for datasets with cell clusters\n",
    "    \"\"\"\n",
    "    sil = []\n",
    "    sil_d0 = []\n",
    "    sil_d3 = []\n",
    "    sil_d7 = []\n",
    "    sil_d11 = []\n",
    "    sil_npc = []\n",
    "\n",
    "    x = np.concatenate((x1_mat,x2_mat))\n",
    "    lab = np.concatenate((x1_lab,x2_lab))\n",
    "\n",
    "    sil_score = silhouette_samples(x,lab)\n",
    "\n",
    "    nsamp = x.shape[0]\n",
    "    for i in range(nsamp):\n",
    "        if(lab[i]==1):\n",
    "            sil_d0.append(sil_score[i])\n",
    "        elif(lab[i]==2):\n",
    "            sil_d3.append(sil_score[i])\n",
    "        elif(lab[i]==3):\n",
    "            sil_d7.append(sil_score[i])\n",
    "        elif(lab[i]==4):\n",
    "            sil_d11.append(sil_score[i])\n",
    "        elif(lab[i]==5):\n",
    "            sil_npc.append(sil_score[i])\n",
    "\n",
    "    avg = np.mean(sil_score)\n",
    "    d0 = sum(sil_d0)/len(sil_d0)\n",
    "    d3 = sum(sil_d3)/len(sil_d3)\n",
    "    d7 = sum(sil_d7)/len(sil_d7)\n",
    "    d11 = sum(sil_d11)/len(sil_d11)\n",
    "    npc = sum(sil_npc)/len(sil_npc)\n",
    "\n",
    "    return avg,d0,d3,d7,d11,npc\n",
    "\n",
    "def binarize_labels(label,x):\n",
    "    \"\"\"\n",
    "    Helper function for calc_auc\n",
    "    \"\"\"\n",
    "    bin_lab = np.array([1] * len(x))\n",
    "    idx = np.where(x == label)\n",
    "\n",
    "    bin_lab[idx] = 0\n",
    "    return bin_lab\n",
    "\n",
    "\n",
    "def calc_auc(x1_mat, x2_mat, x1_lab, x2_lab):\n",
    "    \"\"\"\n",
    "    calculate avg. ROC AUC scores for transformed data when there are >=2 number of clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    nsamp = x1_mat.shape[0]\n",
    "\n",
    "    auc = []\n",
    "    auc_d0 = []\n",
    "    auc_d3 = []\n",
    "    auc_d7 = []\n",
    "    auc_d11 = []\n",
    "    auc_npc = []\n",
    "\n",
    "    for row_idx in range(nsamp):\n",
    "        euc_dist = np.sqrt(np.sum(np.square(np.subtract(x1_mat[row_idx,:], x2_mat)), axis=1))\n",
    "        y_scores = euc_dist\n",
    "        y_true = binarize_labels(x1_lab[row_idx],x2_lab)\n",
    "\n",
    "        auc_score = roc_auc_score(y_true, y_scores)\n",
    "        auc.append(auc_score)\n",
    "        \n",
    "        if(x1_lab[row_idx]==0):\n",
    "            auc_d0.append(auc_score)\n",
    "        elif(x1_lab[row_idx]==1):\n",
    "            auc_d3.append(auc_score)\n",
    "        elif(x1_lab[row_idx]==2):\n",
    "            auc_d7.append(auc_score)\n",
    "        elif(x1_lab[row_idx]==3):\n",
    "            auc_d11.append(auc_score)\n",
    "        elif(x1_lab[row_idx]==4):\n",
    "            auc_npc.append(auc_score)\n",
    "\n",
    "    avg = sum(auc)/len(auc)\n",
    "    d0 = sum(auc_d0)/len(auc_d0)\n",
    "    d3 = sum(auc_d3)/len(auc_d3)\n",
    "    d7 = sum(auc_d7)/len(auc_d7)\n",
    "    d11 = sum(auc_d11)/len(auc_d11)\n",
    "    npc = sum(auc_npc)/len(auc_npc)\n",
    "\n",
    "    return avg,d0,d3,d7,d11,npc\n",
    "\n",
    "def transfer_accuracy(domain1, domain2, type1, type2, n):\n",
    "    \"\"\"\n",
    "    Metric from UnionCom: \"Label Transfer Accuracy\"\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(domain2, type2)\n",
    "    type1_predict = knn.predict(domain1)\n",
    "    np.savetxt(\"type1_predict.txt\", type1_predict)\n",
    "    count = 0\n",
    "    for label1, label2 in zip(type1_predict, type1):\n",
    "        if label1 == label2:\n",
    "            count += 1\n",
    "    return count / len(type1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56cff8e-dc93-49b0-9e0f-3116ede85b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection\n",
    "\n",
    "#Projecting the first domain onto the second domain\n",
    "y_aligned_from_normalized=C2\n",
    "weights_from_normalized=np.sum(P,axis = 0)\n",
    "X_aligned_from_normalized=np.matmul(P, C2) / weights_from_normalized[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba931a56-c9aa-4be6-be21-f48ee6d61e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the average FOSCTTM measure implemented in evals.py for evaluation (metric used in the publication Demetci et al 2021)\n",
    "# This measure reports the fraction of samples closer to a sample than its true match (FOSCTTM), averaged over all samples. \n",
    "fracs_normalized=calc_domainAveraged_FOSCTTM(X_aligned_from_normalized, y_aligned_from_normalized)\n",
    "print(\"Average FOSCTTM score for this alignment with X onto Y is: \", np.mean(fracs_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bb211-c4cc-4325-aed4-60cab95272f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting sorted FOSCTTM to show the distributions of FOSCTTM across cells:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "legend_label=\"SCOT alignment FOSCTTM \\n average value: \"+str(np.mean(fracs_normalized)) #Put average FOSCTTM in the legend\n",
    "plt.plot(np.arange(len(fracs_normalized)), np.sort(fracs_normalized), \"r--\", label=legend_label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Cells\")\n",
    "plt.ylabel(\"Sorted FOSCTTM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75d264-3623-44e7-9f1a-77892a76f4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c37f31-062b-4ce1-9a1f-9db2781b072d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
