{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1ce48-9b86-4939-9bcd-c79c16dc0a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e91118-34ba-4cd0-bd8f-804a20e708f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import metric_learn\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# For optimal transport operations:\n",
    "import ot\n",
    "# For computing graph distances:\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# For pre-processing, normalization\n",
    "from sklearn.preprocessing import StandardScaler, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990db435-c4e6-405e-9329-0e4796794d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatac=np.load(\"../data/scatac_feat.npy\") \n",
    "scrna=np.load(\"../data/scrna_feat.npy\")\n",
    "print(\"Dimensions of input datasets are: \", \"X= \", scatac.shape, \" y= \", scrna.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48aa8c2-de09-4b33-9610-2d4372a8148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2  = normalize(scatac, norm=\"l2\"), normalize(scrna, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8646da-4b6d-4efa-9562-1c2ca9141d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=np.loadtxt(\"../data/SNAREseq_atac_types.txt\")\n",
    "y2=np.loadtxt(\"../data/SNAREseq_rna_types.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241319b3-e52a-41d1-a7cf-82ed6b2faf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77affa72-f3f3-48ee-a310-d5e4bdb02356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262ab08-94f1-48b5-ae24-6b2dee0ef205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c30e9d-394b-4b39-aed9-78a5d67f1e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calcCov(x, y):\n",
    "#     mean_x, mean_y = x.mean(), y.mean()\n",
    "#     n = len(x)\n",
    "#     return sum((x - mean_x) * (y - mean_y)) / n\n",
    "# def cov(data):\n",
    "#     rows, cols = data.shape\n",
    "#     cov_mat = np.zeros((cols, cols))\n",
    " \n",
    "#     for i in range(cols):\n",
    " \n",
    "#         for j in range(cols):\n",
    "#             # store the value in the matrix\n",
    "#             cov_mat[i][j] = calcCov(data[:, i], data[:, j])\n",
    " \n",
    "#     return cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f7662-693e-4ecc-ad40-ddeedba82e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mahalanobis_distance(p1,p2,X): #p1 is model, p2 is the test point\n",
    "#     # X is inverse cov matrix\n",
    "#     distance = np.dot(np.dot(np.subtract(p2,p1).T,np.array(X)),np.subtract(p2,p1))\n",
    "#     return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60612e00-9b32-4b29-a379-ff89b75bed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1cov = cov(X1)\n",
    "# X1cov = np.linalg.inv(X1cov)\n",
    "# distance = mahalanobis_distance(X1[0],X1[1],X1cov)\n",
    "# C1 = np.zeros((X1.shape[0],X1.shape[0]))\n",
    "\n",
    "# for i in range(X1.shape[0]):\n",
    "#     for j in range(X1.shape[0]):\n",
    "#         C1[i][j] = mahalanobis_distance(X1[i],X1[j],X1cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c926208-3ffd-457c-96f3-5eb98bd69919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a748d1f-ec99-46ea-8984-ca7171e3e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2cov = cov(X2) \n",
    "# C2 = np.zeros((X2.shape[0],X2.shape[0]))\n",
    "\n",
    "# for i in range(X2.shape[0]):\n",
    "#     for j in range(X2.shape[0]):\n",
    "#         C2[i][j] = mahalanobis_distance(X2[i],X2[j],X2cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fc927-c660-442d-8253-38c0aa6b6508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaeb869-30e3-4e51-9302-52ba15f759b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting up LMNN\n",
    "X1_metric_learn = metric_learn.RCA()\n",
    "\n",
    "# fit the data!\n",
    "X1_metriclearn = X1_metric_learn.fit_transform(X1, y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322573d8-554f-45c1-a4c7-03dd86a4b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_X1  = X1_metric_learn.get_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15f317-a0b0-4011-a445-c3e1fd4bfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.zeros((X1.shape[0],X1.shape[0]))\n",
    "\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X1.shape[0]):\n",
    "        C1[i][j] = metric_X1(X1[i],X1[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53437d-471b-4dfa-b6e0-b2ba3f0ed60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting up LMNN\n",
    "X2_metric_learn = metric_learn.RCA()\n",
    "\n",
    "# fit the data!\n",
    "X2_metriclearn = X2_metric_learn.fit_transform(X2, y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea72ab-9967-4913-bda6-b065792b2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_X2  = X2_metric_learn.get_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2e0ca-68a4-4600-944a-f88281aa099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C2 = np.zeros((X2.shape[0],X2.shape[0]))\n",
    "\n",
    "for i in range(X2.shape[0]):\n",
    "    for j in range(X2.shape[0]):\n",
    "        C2[i][j] = metric_X2(X2[i],X2[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5cc8a-7c70-482b-a59d-a416b0623260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0620ddd-59ef-44d2-9c79-f5cd369b6804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be6ff5-a84b-40fb-a67d-2940bcf3463c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca1802-513b-4456-8e08-0baeb33346ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9743b-0b61-4182-9be9-5329884839c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4367a-abc1-4c25-97b8-d12f4c9ccf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958acad-127e-4545-9b38-0c6e18d8a295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02b0a7-e47a-4cc1-9478-441f04ac95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import torch\n",
    "\n",
    "import ot\n",
    "from ot.gromov import gromov_wasserstein2\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def min_weight_gw(C1, C2, a2, nb_iter_max=100, lr=1e-2):\n",
    "    \"\"\" solve min_a GW(C1,C2,a, a2) by gradient descent\"\"\"\n",
    "\n",
    "    # use pyTorch for our data\n",
    "    C1_torch = torch.tensor(C1)\n",
    "    C2_torch = torch.tensor(C2)\n",
    "\n",
    "    a0 = rng.rand(C1.shape[0])  # random_init\n",
    "    a0 /= a0.sum()  # on simplex\n",
    "    a1_torch = torch.tensor(a0).requires_grad_(True)\n",
    "    a2_torch = torch.tensor(a2)\n",
    "\n",
    "    loss_iter = []\n",
    "\n",
    "    for i in range(nb_iter_max):\n",
    "\n",
    "        loss = gromov_wasserstein2(C1_torch, C2_torch, a1_torch, a2_torch)\n",
    "\n",
    "        loss_iter.append(loss.clone().detach().cpu().numpy())\n",
    "        loss.backward()\n",
    "\n",
    "        #print(\"{:03d} | {}\".format(i, loss_iter[-1]))\n",
    "\n",
    "        # performs a step of projected gradient descent\n",
    "        with torch.no_grad():\n",
    "            grad = a1_torch.grad\n",
    "            a1_torch -= grad * lr   # step\n",
    "            a1_torch.grad.zero_()\n",
    "            a1_torch.data = ot.utils.proj_simplex(a1_torch)\n",
    "\n",
    "    a1 = a1_torch.clone().detach().cpu().numpy()\n",
    "\n",
    "    return a1, loss_iter\n",
    "\n",
    "\n",
    "a0_est, loss_iter0 = min_weight_gw(C1, C2, ot.unif(C1.shape[0]), nb_iter_max=10, lr=1e-2)\n",
    "\n",
    "\n",
    "pl.figure(2)\n",
    "pl.plot(loss_iter0)\n",
    "pl.title(\"Loss along iterations\")\n",
    "\n",
    "print(\"Estimated weights : \", a0_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7287dff-e7f9-4ef5-befb-c8618498c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0_est.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad14da-3824-4ad9-a4f8-9b43244b76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph(x, C, color='C0', s=None):\n",
    "    for j in range(C.shape[0]):\n",
    "        for i in range(j):\n",
    "            if C[i, j] > 0:\n",
    "                pl.plot([x[i, 0], x[j, 0]], [x[i, 1], x[j, 1]], alpha=0.2, color='k')\n",
    "    pl.scatter(x[:, 0], x[:, 1], c=color, s=s, zorder=10, edgecolors='k', cmap='tab10', vmax=9)\n",
    "\n",
    "    \n",
    "T_unif = ot.gromov_wasserstein(C2, C1, ot.unif(C1.shape[0]), ot.unif(C2.shape[0]))\n",
    "label_unif = T_unif.argmax(1)\n",
    "\n",
    "T_est = ot.gromov_wasserstein(C2, C1, ot.unif(C1.shape[0]), a0_est)\n",
    "label_est = T_est.argmax(1)\n",
    "\n",
    "# get 2d position for nodes\n",
    "x1 = MDS(dissimilarity='precomputed', random_state=0).fit_transform(1 - C1)\n",
    "\n",
    "# pl.figure(3, (10, 5))\n",
    "# pl.clf()\n",
    "# pl.subplot(1, 2, 1)\n",
    "# plot_graph(x1, C1, color=label_unif)\n",
    "# pl.title(\"Graph clustering unif. weights\")\n",
    "# pl.axis(\"off\")\n",
    "# pl.subplot(1, 2, 2)\n",
    "# plot_graph(x1, C1, color=label_est)\n",
    "# pl.title(\"Graph clustering est. weights\")\n",
    "# pl.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102eba73-9e57-4108-b806-97823d418947",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_unif.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78ca66-08db-4215-b4dc-483c3d6dda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_est.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d42685-10d3-4226-b60f-6f45b9eee876",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_unif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e890fc-6910-4cc1-98de-aee17721e3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6488325-1dcf-4d9b-aabf-c79fdf11fd08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4fe74-0218-4d7c-9d50-3eadef0569a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_compession_gw(nb_nodes, C2, a2, nb_iter_max=100, lr=1e-2):\n",
    "    \"\"\" solve min_a GW(C1,C2,a, a2) by gradient descent\"\"\"\n",
    "\n",
    "    # use pyTorch for our data\n",
    "\n",
    "    C2_torch = torch.tensor(C2)\n",
    "    a2_torch = torch.tensor(a2)\n",
    "\n",
    "    a0 = rng.rand(nb_nodes)  # random_init\n",
    "    a0 /= a0.sum()  # on simplex\n",
    "    a1_torch = torch.tensor(a0).requires_grad_(True)\n",
    "    C0 = np.eye(nb_nodes)\n",
    "    C1_torch = torch.tensor(C0).requires_grad_(True)\n",
    "\n",
    "    loss_iter = []\n",
    "\n",
    "    for i in range(nb_iter_max):\n",
    "\n",
    "        loss = gromov_wasserstein2(C1_torch, C2_torch, a1_torch, a2_torch)\n",
    "\n",
    "        loss_iter.append(loss.clone().detach().cpu().numpy())\n",
    "        loss.backward()\n",
    "\n",
    "        #print(\"{:03d} | {}\".format(i, loss_iter[-1]))\n",
    "\n",
    "        # performs a step of projected gradient descent\n",
    "        with torch.no_grad():\n",
    "            grad = a1_torch.grad\n",
    "            a1_torch -= grad * lr   # step\n",
    "            a1_torch.grad.zero_()\n",
    "            a1_torch.data = ot.utils.proj_simplex(a1_torch)\n",
    "\n",
    "            grad = C1_torch.grad\n",
    "            C1_torch -= grad * lr   # step\n",
    "            C1_torch.grad.zero_()\n",
    "            C1_torch.data = torch.clamp(C1_torch, 0, 1)\n",
    "\n",
    "    a1 = a1_torch.clone().detach().cpu().numpy()\n",
    "    C1 = C1_torch.clone().detach().cpu().numpy()\n",
    "\n",
    "    return a1, C1, loss_iter\n",
    "\n",
    "\n",
    "# nb_nodes = C2.shape[0]\n",
    "# a0_est2, C0_est2, loss_iter2 = graph_compession_gw(nb_nodes, C2, ot.unif(C2.shape[0]),\n",
    "#                                                    nb_iter_max=300, lr=5e-2)\n",
    "\n",
    "# pl.figure(4)\n",
    "# pl.plot(loss_iter2)\n",
    "# pl.title(\"Loss along iterations\")\n",
    "\n",
    "\n",
    "# print(\"Estimated weights : \", a0_est2)\n",
    "\n",
    "# pl.figure(6, (10, 3.5))\n",
    "# pl.clf()\n",
    "# pl.imshow(C0_est2, vmin=0, vmax=1)\n",
    "# pl.title('Estimated C0 matrix')\n",
    "# pl.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0015d5-8eda-4d03-b7f2-d0a42c7f6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a0_est2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477f947-b472-48e1-b18c-5d090332fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C0_est2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0b0f8-04aa-4d7d-a043-f68b4288b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval function\n",
    "\n",
    "import numpy as np\n",
    "import random, math, os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import roc_auc_score, silhouette_samples\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def calc_frac_idx(x1_mat,x2_mat):\n",
    "    \"\"\"\n",
    "    Returns fraction closer than true match for each sample (as an array)\n",
    "    \"\"\"\n",
    "    fracs = []\n",
    "    x = []\n",
    "    nsamp = x1_mat.shape[0]\n",
    "    rank=0\n",
    "    for row_idx in range(nsamp):\n",
    "        euc_dist = np.sqrt(np.sum(np.square(np.subtract(x1_mat[row_idx,:], x2_mat)), axis=1))\n",
    "        true_nbr = euc_dist[row_idx]\n",
    "        sort_euc_dist = sorted(euc_dist)\n",
    "        rank =sort_euc_dist.index(true_nbr)\n",
    "        frac = float(rank)/(nsamp -1)\n",
    "\n",
    "        fracs.append(frac)\n",
    "        x.append(row_idx+1)\n",
    "\n",
    "    return fracs,x\n",
    "\n",
    "def calc_domainAveraged_FOSCTTM(x1_mat, x2_mat):\n",
    "    \"\"\"\n",
    "    Outputs average FOSCTTM measure (averaged over both domains)\n",
    "    Get the fraction matched for all data points in both directions\n",
    "    Averages the fractions in both directions for each data point\n",
    "    \"\"\"\n",
    "    \n",
    "    fracs1,xs = calc_frac_idx(x1_mat, x2_mat)\n",
    "    fracs2,xs = calc_frac_idx(x2_mat, x1_mat)\n",
    "    fracs = []\n",
    "    for i in range(len(fracs1)):\n",
    "        fracs.append((fracs1[i]+fracs2[i])/2)  \n",
    "    return fracs\n",
    "\n",
    "def calc_sil(x1_mat,x2_mat,x1_lab,x2_lab):\n",
    "    \"\"\"\n",
    "    Returns silhouette score for datasets with cell clusters\n",
    "    \"\"\"\n",
    "    sil = []\n",
    "    sil_d0 = []\n",
    "    sil_d3 = []\n",
    "    sil_d7 = []\n",
    "    sil_d11 = []\n",
    "    sil_npc = []\n",
    "\n",
    "    x = np.concatenate((x1_mat,x2_mat))\n",
    "    lab = np.concatenate((x1_lab,x2_lab))\n",
    "\n",
    "    sil_score = silhouette_samples(x,lab)\n",
    "\n",
    "    nsamp = x.shape[0]\n",
    "    for i in range(nsamp):\n",
    "        if(lab[i]==1):\n",
    "            sil_d0.append(sil_score[i])\n",
    "        elif(lab[i]==2):\n",
    "            sil_d3.append(sil_score[i])\n",
    "        elif(lab[i]==3):\n",
    "            sil_d7.append(sil_score[i])\n",
    "        elif(lab[i]==4):\n",
    "            sil_d11.append(sil_score[i])\n",
    "        elif(lab[i]==5):\n",
    "            sil_npc.append(sil_score[i])\n",
    "\n",
    "    avg = np.mean(sil_score)\n",
    "    d0 = sum(sil_d0)/len(sil_d0)\n",
    "    d3 = sum(sil_d3)/len(sil_d3)\n",
    "    d7 = sum(sil_d7)/len(sil_d7)\n",
    "    d11 = sum(sil_d11)/len(sil_d11)\n",
    "    npc = sum(sil_npc)/len(sil_npc)\n",
    "\n",
    "    return avg,d0,d3,d7,d11,npc\n",
    "\n",
    "def binarize_labels(label,x):\n",
    "    \"\"\"\n",
    "    Helper function for calc_auc\n",
    "    \"\"\"\n",
    "    bin_lab = np.array([1] * len(x))\n",
    "    idx = np.where(x == label)\n",
    "\n",
    "    bin_lab[idx] = 0\n",
    "    return bin_lab\n",
    "\n",
    "\n",
    "def calc_auc(x1_mat, x2_mat, x1_lab, x2_lab):\n",
    "    \"\"\"\n",
    "    calculate avg. ROC AUC scores for transformed data when there are >=2 number of clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    nsamp = x1_mat.shape[0]\n",
    "\n",
    "    auc = []\n",
    "    auc_d0 = []\n",
    "    auc_d3 = []\n",
    "    auc_d7 = []\n",
    "    auc_d11 = []\n",
    "    auc_npc = []\n",
    "\n",
    "    for row_idx in range(nsamp):\n",
    "        euc_dist = np.sqrt(np.sum(np.square(np.subtract(x1_mat[row_idx,:], x2_mat)), axis=1))\n",
    "        y_scores = euc_dist\n",
    "        y_true = binarize_labels(x1_lab[row_idx],x2_lab)\n",
    "\n",
    "        auc_score = roc_auc_score(y_true, y_scores)\n",
    "        auc.append(auc_score)\n",
    "        \n",
    "        if(x1_lab[row_idx]==0):\n",
    "            auc_d0.append(auc_score)\n",
    "        elif(x1_lab[row_idx]==1):\n",
    "            auc_d3.append(auc_score)\n",
    "        elif(x1_lab[row_idx]==2):\n",
    "            auc_d7.append(auc_score)\n",
    "        elif(x1_lab[row_idx]==3):\n",
    "            auc_d11.append(auc_score)\n",
    "        elif(x1_lab[row_idx]==4):\n",
    "            auc_npc.append(auc_score)\n",
    "\n",
    "    avg = sum(auc)/len(auc)\n",
    "    d0 = sum(auc_d0)/len(auc_d0)\n",
    "    d3 = sum(auc_d3)/len(auc_d3)\n",
    "    d7 = sum(auc_d7)/len(auc_d7)\n",
    "    d11 = sum(auc_d11)/len(auc_d11)\n",
    "    npc = sum(auc_npc)/len(auc_npc)\n",
    "\n",
    "    return avg,d0,d3,d7,d11,npc\n",
    "\n",
    "def transfer_accuracy(domain1, domain2, type1, type2, n):\n",
    "    \"\"\"\n",
    "    Metric from UnionCom: \"Label Transfer Accuracy\"\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(domain2, type2)\n",
    "    type1_predict = knn.predict(domain1)\n",
    "    np.savetxt(\"type1_predict.txt\", type1_predict)\n",
    "    count = 0\n",
    "    for label1, label2 in zip(type1_predict, type1):\n",
    "        if label1 == label2:\n",
    "            count += 1\n",
    "    return count / len(type1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe510e68-ff2f-48f5-bf04-ca9034dda586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection\n",
    "\n",
    "#Projecting the first domain onto the second domain\n",
    "y_aligned_from_normalized=X2\n",
    "weights_from_normalized=np.sum(T_unif,axis = 0)\n",
    "X_aligned_from_normalized=np.matmul(T_unif, X2) / weights_from_normalized[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e11cca-e184-411b-849c-96e6e6d1c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the average FOSCTTM measure implemented in evals.py for evaluation (metric used in the publication Demetci et al 2021)\n",
    "# This measure reports the fraction of samples closer to a sample than its true match (FOSCTTM), averaged over all samples. \n",
    "fracs_normalized=calc_domainAveraged_FOSCTTM(X_aligned_from_normalized, y_aligned_from_normalized)\n",
    "print(\"Average FOSCTTM score for this alignment with X onto Y is: \", np.mean(fracs_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379a8e9-270c-4c9b-bdcf-5f5af2e7d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting sorted FOSCTTM to show the distributions of FOSCTTM across cells:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "legend_label=\"SCOT alignment FOSCTTM \\n average value: \"+str(np.mean(fracs_normalized)) #Put average FOSCTTM in the legend\n",
    "plt.plot(np.arange(len(fracs_normalized)), np.sort(fracs_normalized), \"r--\", label=legend_label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Cells\")\n",
    "plt.ylabel(\"Sorted FOSCTTM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe01fcc-dc59-4b4f-a551-f2042260ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Reduce the dimensionality of the aligned domains to two (2D) via PCA for the sake of visualization:\n",
    "pca=PCA(n_components=2)\n",
    "Xy_pca=pca.fit_transform(np.concatenate((X_aligned_from_normalized, y_aligned_from_normalized), axis=0))\n",
    "X_pca=Xy_pca[0: 1047,]\n",
    "y_pca=Xy_pca[1047:,]\n",
    "\n",
    "#Plot aligned domains, samples colored by domain identity:\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=\"k\", s=15, label=\"Chromatin Accessibility\")\n",
    "plt.scatter(y_pca[:,0], y_pca[:,1], c=\"r\", s=15, label=\"Gene Expression\")\n",
    "plt.legend()\n",
    "plt.title(\"Colored based on domains\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure\n",
    "#Plot aligned domains, samples colored by domain identity:\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=\"k\", s=15, label=\"Chromatin Accessibility\")\n",
    "plt.legend()\n",
    "plt.title(\"Colored based on domains\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure\n",
    "#Plot aligned domains, samples colored by domain identity:\n",
    "plt.scatter(y_pca[:,0], y_pca[:,1], c=\"r\", s=15, label=\"Gene Expression\")\n",
    "plt.legend()\n",
    "plt.title(\"Colored based on domains\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fee1f8-a90b-45fe-9037-c26a5cfb45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot aligned domains, samples colored by cell types:\n",
    "cellTypes_atac=np.loadtxt(\"../data/SNAREseq_atac_types.txt\")\n",
    "cellTypes_rna=np.loadtxt(\"../data/SNAREseq_rna_types.txt\")\n",
    "\n",
    "colormap = plt.get_cmap('rainbow', 4) \n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=cellTypes_atac, s=15, cmap=colormap)\n",
    "plt.scatter(y_pca[:,0], y_pca[:,1], c=cellTypes_rna, s=15, cmap=colormap)\n",
    "# plt.colorbar()\n",
    "cbar=plt.colorbar()\n",
    "\n",
    "# approximately center the colors on the colorbar when adding cell type labels\n",
    "tick_locs = (np.arange(1,5)+0.75) *3/4 \n",
    "cbar.set_ticks(tick_locs)\n",
    "cbar.set_ticklabels([\"H1\", \"GM\", \"BJ\", \"K562\"]) #cell-type labels\n",
    "plt.title(\"Colored based on cell type identity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c64b74-e6eb-4b51-92a8-98bf8165e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalX_pca=pca.fit_transform(X1)\n",
    "originaly_pca=pca.fit_transform(X2)\n",
    "\n",
    "#Visualization of the global geometry\n",
    "fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "ax1.scatter(originalX_pca[:,0], originalX_pca[:,1], c=\"k\", s=15)\n",
    "ax1.set_title(\"Chromatin Accessibiliy Domain \\n *before* Alignment\")\n",
    "ax2.scatter(originaly_pca[:,0], originaly_pca[:,1], c=\"r\", s=15)\n",
    "ax2.set_title(\"Gene Expression Domain \\n *before* Alignment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bd02f-e994-4a8a-bd6d-b2536d1ef93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of the cell type clusters in original domains *before* alignment\n",
    "fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "\n",
    "fig1= ax1.scatter(originalX_pca[:,0], originalX_pca[:,1], c=cellTypes_atac, s=15, cmap=colormap)\n",
    "ax1.set_title(\"Chromatin Accessibiliy \\n *before* Alignment\")\n",
    "\n",
    "fig2= ax2.scatter(originaly_pca[:,0], originaly_pca[:,1],  c=cellTypes_rna, s=15, cmap=colormap)\n",
    "ax2.set_title(\"Gene Expression Domain \\n *before* Alignment\")\n",
    "\n",
    "cbar=fig.colorbar(fig2)\n",
    "cbar.set_ticks(tick_locs)\n",
    "cbar.set_ticklabels([\"H1\", \"GM\", \"BJ\", \"K562\"]) #cell-type labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e35b46-9b65-4d4b-ae4b-aab06f27ecd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3182f-dde0-4556-b49b-6ff76f444ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f97eae-c4ef-4d95-9bdf-6841210bb8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8d8f6-25e4-427f-81bc-2eb158da4282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ba9a3-5862-4f2a-9d9f-d242e4a6ffe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4647dcb8-01b9-4bcb-9a82-bb277b335a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7396, -0.0272, -0.2027,  ..., -0.6204,  1.3295,  1.1549],\n",
      "        [-1.3363, -0.6620,  1.0288,  ...,  0.8893, -1.4013, -2.2258],\n",
      "        [-1.0180, -1.7433, -0.7344,  ...,  1.5548, -0.1579,  0.0903],\n",
      "        ...,\n",
      "        [ 0.7112, -0.5546,  2.0906,  ..., -0.2433,  1.1326,  0.3307],\n",
      "        [-0.7007, -0.3370,  1.1479,  ...,  0.1687,  2.4663, -0.7770],\n",
      "        [ 0.8166,  0.6741, -2.1385,  ...,  0.6308,  1.0232, -0.3835]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import miners, losses\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the input tensor\n",
    "A = torch.randn(100, 100)\n",
    "\n",
    "# Fix the first 50 rows of A\n",
    "fixed_rows = A[:50, :]\n",
    "\n",
    "# Define the variable tensor for the last 50 rows of A\n",
    "var_rows = torch.randn(50, 100)\n",
    "\n",
    "# Define the Miner\n",
    "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "\n",
    "# Define the ProxyAnchorLoss\n",
    "loss_fn = losses.ProxyAnchorLoss(num_classes=50, embedding_size=100)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD([var_rows], lr=0.01)\n",
    "\n",
    "# Perform the optimization\n",
    "num_iterations = 1000\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Mine the hard negatives\n",
    "    hard_pairs = miner(var_rows, torch.tensor([i for i in range(50)]))\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(var_rows, torch.tensor([i for i in range(50)]), hard_pairs)\n",
    "    \n",
    "    # Add a penalty to the loss to encourage the output to be close to the fixed rows\n",
    "    penalty = F.mse_loss(var_rows, fixed_rows)\n",
    "    loss += penalty\n",
    "    \n",
    "    # Perform backpropagation and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Print the optimized output\n",
    "print(var_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec13e4-7b3e-43ec-b75e-7839b1d808f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96b58d-1572-4651-87f8-e7db5026ef14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7f39e-956d-4ae1-9fb2-945285d3ed2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91a6cf-feeb-4f43-b6ce-a0b112d64934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3eda66-b2b6-465d-a5a0-824440be62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thnguy22/opt/anaconda3/envs/singlecell/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2575, -0.4642,  1.6648,  ...,  0.3470, -0.9651,  0.7426],\n",
      "        [ 0.9870, -0.2222,  0.3301,  ..., -0.3669,  0.0924,  0.2013],\n",
      "        [-1.4439, -0.4751, -0.0994,  ...,  0.4002,  2.0583, -1.3915],\n",
      "        ...,\n",
      "        [ 0.2864, -1.1707, -0.3838,  ...,  0.3727, -0.7677,  0.0377],\n",
      "        [-1.1144, -0.7462, -0.2922,  ...,  0.2014,  0.3760,  1.9066],\n",
      "        [ 1.2016,  3.7003, -0.2942,  ..., -1.2272, -1.2431, -0.2215]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import miners, losses\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn, optim\n",
    "import pytorch_metric_learning.losses as loss\n",
    "import pytorch_metric_learning.miners as miners\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Define the input tensor\n",
    "A = torch.randn(100, 100)\n",
    "B = torch.randn(100,100)\n",
    "\n",
    "# perform k-means clustering with k=2\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(A.numpy())\n",
    "\n",
    "# Fix the first 50 rows of A\n",
    "fixed_rows = A[:50, :]\n",
    "\n",
    "# Define the variable tensor for the last 50 rows of A\n",
    "var_rows = torch.randn(50, 100)\n",
    "\n",
    "# Define the Miner\n",
    "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "\n",
    "# Define the ProxyAnchorLoss\n",
    "loss_fn = losses.ProxyAnchorLoss(num_classes=50, embedding_size=100)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD([var_rows], lr=0.01)\n",
    "\n",
    "# Perform the optimization\n",
    "num_iterations = 1000\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Mine the hard negatives\n",
    "    hard_pairs = miner(var_rows, torch.tensor([i for i in range(50)]))\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(var_rows, torch.tensor([i for i in range(50)]), hard_pairs)\n",
    "    \n",
    "    # Add a penalty to the loss to encourage the output to be close to the fixed rows\n",
    "    penalty = F.mse_loss(var_rows, fixed_rows)\n",
    "    loss += penalty\n",
    "    \n",
    "    # Perform backpropagation and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Print the optimized output\n",
    "print(var_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b83baf-8636-4062-8105-d4bca9c54d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ea57c-d619-4d7c-8f61-b915664accec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e18975-c42c-454c-a55d-67b08db51601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b8652-9ad8-477f-b956-92fb65036af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc2d1f-4744-4f97-944a-f3e457eac87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f035b4-168a-4083-8005-bc10f0018084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
